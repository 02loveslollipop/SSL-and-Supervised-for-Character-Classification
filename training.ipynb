{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9kdcGDU0LIN"
   },
   "source": [
    "# Comparative Analysis of Self-Supervised and Supervised Pretraining Approaches for Genshin Impact Character Classification Using ResNet-18\n",
    "\n",
    "This notebook implements an experimental framework to compare self-supervised and supervised pretraining approaches for the task of Genshin Impact character classification using the ResNet-18 architecture. The goal is to evaluate the effectiveness of different pretraining strategies in leveraging limited labeled data for improved classification performance.\n",
    "\n",
    "## Required Libraries\n",
    "- PyTorch\n",
    "- Torchvision\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Pandas\n",
    "- Scikit-learn\n",
    "- Seaborn\n",
    "- Hugging Face (Optional for model uploading)\n",
    "- Kaggle (for dataset management)\n",
    "\n",
    "## Key elements of the implementation:\n",
    "- **Image Preprocessing**:\n",
    "    - Image resizing to 256x256 pixels with stretching and no aspect ratio preservation.\n",
    "    - Normalization using ImageNet statistics.\n",
    "\n",
    "- **Data Augmentation**:\n",
    "    - Random cropping, horizontal flipping, color jittering, and Gaussian blur for SimCLR pretraining.\n",
    "    - Random scaling $[1.0, 1.5]$ with random center cropping, rotation $[-15^\\circ, 15^\\circ]$ and horizontal flipping for fine-tuning.\n",
    "\n",
    "- **Model Architecture**:\n",
    "    - ResNet-18 with a projection head for SimCLR pretraining.\n",
    "    - Classification MLP head for fine-tuning with 6 output classes.\n",
    "\n",
    "- **Training Configuration**:\n",
    "    - SimCLR pretraining with 500 epochs, batch size of 256, and a learning rate of 0.5.\n",
    "    - Fine-tuning with 100 epochs, batch size of 32, and a learning rate of 0.01, with gradual unfreezing ResNet layers.\n",
    "\n",
    "- **Loss Functions**:\n",
    "    - Contrastive loss with temperature scaling for SimCLR pretraining.\n",
    "    - Cross-entropy loss for classification fine-tuning.\n",
    "\n",
    "> The contrastive loss is computed using the cosine similarity between the projected features of positive pairs, while the classification loss is computed using the softmax output of the final classification layer.\n",
    "\n",
    "\n",
    "- **Evaluation Metrics**:\n",
    "    - Cross-validation with 5 folds to ensure robustness.\n",
    "    - Top-1 and Top-5 accuracy, F1-score, precision, recall, and confusion matrix analysis.\n",
    "    - Visualization of learned features using t-SNE and Grad-CAM for interpretability.\n",
    "\n",
    "- **Models Training Approaches**:\n",
    "    - Pure supervised training on final dataset. (Comparison baseline)\n",
    "    - Self-supervised pretraining on unlabeled dataset followed by fine-tuning by supervised training on the final dataset.\n",
    "    - Self-supervised pretraining on unlabeled dataset followed by fine-tuning by semi-supervised training on the final dataset + non labeled dataset.\n",
    "    - ImageNet supervised pretraining followed by fine-tuning by supervised training on the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j0g1AzB_qrb"
   },
   "source": [
    "## 1. Dependencies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "78r7e2ji_ujD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVEA0DV7PTpl"
   },
   "source": [
    "## 2.Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADuABP22Pdsu",
    "outputId": "296e71a0-9267-4cd4-b9b5-de1d22a7b20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memory Available: 4.00 GB\n",
      "Memory Allocated: 0.00 GB\n",
      "Memory Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"Memory Cached: {torch.cuda.memory_reserved(0) / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF32 for cuDNN: False\n",
      "TF32 for matmul: False\n",
      "Default tensor dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "print(f\"TF32 for cuDNN: {torch.backends.cudnn.allow_tf32}\")\n",
    "print(f\"TF32 for matmul: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "print(f\"Default tensor dtype: {torch.get_default_dtype()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQI9PuZPPjzK",
    "outputId": "1b4febf3-a636-439a-a8f6-bdb9dd099a5b"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj3aBi5t_Rtq"
   },
   "source": [
    "## 3. Dataset retrieval and transformation for Self-Supervised Learning dataset\n",
    "- Download the Genshin Impact character dataset from Kaggle.\n",
    "- Apply image transformations.\n",
    "- Save as a single dataset for self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "SU8ZdMR2yc_4",
    "outputId": "76f36061-336f-4351-e26f-c92f3e94487a"
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406] #Using ImageNet mean and std for normalization\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "N = 10\n",
    "\n",
    "if os.path.exists(os.path.join(os.getcwd(), \"datasets\", \"ssl-dataset.pt\")): # check if the combined dataset exists\n",
    "    print(\"Combined dataset already exists. Loading...\")\n",
    "    combined_dataset = torch.load(os.path.join(os.getcwd(), \"datasets\", \"ssl-dataset.pt\"), weights_only=False)\n",
    "else: #if not, download, process the datasets and save the combined dataset\n",
    "    print(\"Combined dataset does not exist. Downloading and processing datasets...\")\n",
    "\n",
    "    ds1_path = kagglehub.dataset_download(\"soumikrakshit/anime-faces\")\n",
    "    ds2_path = kagglehub.dataset_download(\"stevenevan99/face-of-pixiv-top-daily-illustration-2020\")\n",
    "    ds3_path = kagglehub.dataset_download(\"hirunkulphimsiri/fullbody-anime-girls-datasets\")\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), #256x256px resize\n",
    "        transforms.ToTensor(), #convert to tensor\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) #normalize with ImageNet stats\n",
    "    ])\n",
    "\n",
    "    # Load the datasets\n",
    "    dataset1 = datasets.ImageFolder(root=ds1_path, transform=transform)\n",
    "    dataset2 = datasets.ImageFolder(root=ds2_path, transform=transform)\n",
    "    dataset3 = datasets.ImageFolder(root=ds3_path, transform=transform)\n",
    "    # Combine the datasets\n",
    "    combined_dataset = torch.utils.data.ConcatDataset([dataset1, dataset2, dataset3])\n",
    "    # Save the combined dataset\n",
    "    combined_dataset_path = os.path.join(os.getcwd(), \"datasets\")\n",
    "    if not os.path.exists(combined_dataset_path):\n",
    "        os.makedirs(combined_dataset_path)\n",
    "    torch.save(combined_dataset, os.path.join(combined_dataset_path, \"ssl-dataset.pt\"))\n",
    "\n",
    "# Visualize n random images from the combined dataset\n",
    "def visualize_dataset(dataset, num_images=5):\n",
    "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "    images = [dataset[i][0] for i in indices]\n",
    "    labels = [dataset[i][1] for i in indices]\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(F.to_pil_image(img))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "    plt.title(f\"Sample Images from Combined Dataset ({num_images} images)\")\n",
    "    plt.show()\n",
    "visualize_dataset(combined_dataset, num_images=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz8m0jvdPx04"
   },
   "source": [
    "## 4. Dataset retrieval and transformation for Supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3Rz1s4JOdqU",
    "outputId": "8f364553-a5b3-43ef-8aa6-92b23fa0057e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive # Mount Google Drive to access dataset\n",
    "drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "TDa9pu3CQkAp",
    "outputId": "9e80b84e-b03a-47e3-8d6a-d68f3efceba4"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "N = 10 # Number of images to visualize\n",
    "\n",
    "dataset_path_compressed = \"/content/gdrive/MyDrive/GenshinImageClassifier/dataset.zip\"\n",
    "\n",
    "#check if the processed dataset exists\n",
    "if os.path.exists(os.path.join(os.getcwd(), \"datasets\", \"finetune-dataset.pt\")):\n",
    "    print(\"Processed dataset already exists. Loading...\")\n",
    "    dataset = torch.load(os.path.join(os.getcwd(), \"datasets\", \"finetune-dataset.pt\"), weights_only=False)\n",
    "else:\n",
    "    #check if non procceessed dataset exists\n",
    "    if not os.path.exists(dataset_path_compressed):\n",
    "        print(f\"Dataset file at {dataset_path_compressed} does not exist. Please download it or update the path.\")\n",
    "\n",
    "    # Copy it to the current working directory in /tmp/ folder if it does not exist, create it\n",
    "    if not os.path.exists(os.getcwd() + \"/tmp/\"):\n",
    "        os.makedirs(os.getcwd() + \"/tmp/\")\n",
    "\n",
    "    print(f\"Copying dataset file to /tmp/ directory...\")\n",
    "    shutil.copy(dataset_path_compressed, os.getcwd() + \"/tmp/dataset.zip\")\n",
    "\n",
    "    # Unzip the dataset\n",
    "    with zipfile.ZipFile(os.getcwd() + \"/tmp/dataset.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.getcwd() + \"/tmp/\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset_path = os.path.join(os.getcwd(), \"tmp\", \"dataset\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Dataset path {dataset_path} does not exist. Please check the extraction.\")\n",
    "\n",
    "    # Load the dataset using ImageFolder\n",
    "    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "    # Save as .pt file\n",
    "    torch.save(dataset, os.path.join(os.getcwd(), \"datasets\", \"finetune-dataset.pt\"))\n",
    "\n",
    "# Visualize n random images from the dataset\n",
    "visualize_dataset(dataset, num_images=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEZpG5HYWUnj"
   },
   "source": [
    "## 5. Models training, evaluation and feature extraction/visualization\n",
    "- Train the baseline model on the final dataset.\n",
    "- Train the self-supervised model on the unlabeled dataset.\n",
    "- Fine-tune the self-supervised model on the final dataset using supervised training.\n",
    "- Fine-tune the self-supervised model on the final dataset using semi-supervised training.\n",
    "- Fine-tune the ImageNet pre-trained model on the final dataset using supervised training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mof62Jz7Ztq9"
   },
   "source": [
    "### 5.1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrdLE45ndgeH",
    "outputId": "19f5455b-b979-46e1-8168-d8a7fa77c2eb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "K_FOLDS = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Disable TF32 to ensure only FP16 is used for matmul/convolutions\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# Enable FP16 operations to ensure all operations are performed in FP16\n",
    "torch.backends.cudnn.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "\n",
    "# Enable Autotuner for cuDNN for better performance on convolutions\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Get all indices for the dataset\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "print(f\"Starting {K_FOLDS}-fold cross-validation on {dataset_size} samples...\")\n",
    "print(\"Training with pure FP16 (Tensor and CUDA cores). Model weights stored as FP32.\")\n",
    "\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "fold_results = {\n",
    "    'train_losses': [],\n",
    "    'train_accuracies': [],\n",
    "    'val_losses': [],\n",
    "    'val_accuracies': [],\n",
    "    'test_metrics': []\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{K_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, sampler=train_sampler\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, sampler=val_sampler\n",
    "    )\n",
    "\n",
    "    # Initialize model and convert to FP16 for training\n",
    "    model = models.resnet18(weights=None, num_classes=6).to(device)\n",
    "    model = model.half()  # Convert model to FP16\n",
    "\n",
    "    print(f\"Model parameters dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "    loss_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_val_losses = []\n",
    "    fold_val_accuracies = []\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.half().to(device)  # Convert input to FP16\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_idx)\n",
    "        epoch_train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.half().to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_idx)\n",
    "        epoch_val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        fold_train_losses.append(epoch_train_loss)\n",
    "        fold_train_accuracies.append(epoch_train_accuracy)\n",
    "        fold_val_losses.append(epoch_val_loss)\n",
    "        fold_val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            # Store model weights as FP32 for stability\n",
    "            best_model_state = {k: v.float().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "            print(f\"  Train - Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_accuracy:.2f}%\")\n",
    "            print(f\"  Val   - Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_accuracy:.2f}%\")\n",
    "\n",
    "    # Load best model weights (convert back to FP16 for evaluation)\n",
    "    model.load_state_dict({k: v.to(device).half() for k, v in best_model_state.items()})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.half().to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    fold_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    fold_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    fold_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    fold_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    fold_results['train_losses'].append(fold_train_losses)\n",
    "    fold_results['train_accuracies'].append(fold_train_accuracies)\n",
    "    fold_results['val_losses'].append(fold_val_losses)\n",
    "    fold_results['val_accuracies'].append(fold_val_accuracies)\n",
    "    fold_results['test_metrics'].append({\n",
    "        'accuracy': fold_accuracy,\n",
    "        'f1': fold_f1,\n",
    "        'precision': fold_precision,\n",
    "        'recall': fold_recall\n",
    "    })\n",
    "\n",
    "    print(f\"\\nFold {fold + 1} Results:\")\n",
    "    print(f\"  Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    print(f\"  Test Accuracy: {fold_accuracy*100:.2f}%\")\n",
    "    print(f\"  Test F1-Score: {fold_f1:.4f}\")\n",
    "    print(f\"  Test Precision: {fold_precision:.4f}\")\n",
    "    print(f\"  Test Recall: {fold_recall:.4f}\")\n",
    "\n",
    "# Calculate and display overall results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Extract test metrics\n",
    "test_accuracies = [fold['accuracy'] for fold in fold_results['test_metrics']]\n",
    "test_f1_scores = [fold['f1'] for fold in fold_results['test_metrics']]\n",
    "test_precisions = [fold['precision'] for fold in fold_results['test_metrics']]\n",
    "test_recalls = [fold['recall'] for fold in fold_results['test_metrics']]\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"Test Accuracy:  {np.mean(test_accuracies)*100:.2f}% ± {np.std(test_accuracies)*100:.2f}%\")\n",
    "print(f\"Test F1-Score:  {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "print(f\"Test Precision: {np.mean(test_precisions):.4f} ± {np.std(test_precisions):.4f}\")\n",
    "print(f\"Test Recall:    {np.mean(test_recalls):.4f} ± {np.std(test_recalls):.4f}\")\n",
    "\n",
    "# Plot training curves for all folds\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Average training loss\n",
    "avg_train_losses = np.mean(fold_results['train_losses'], axis=0)\n",
    "std_train_losses = np.std(fold_results['train_losses'], axis=0)\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "axes[0, 0].plot(epochs_range, avg_train_losses, 'b-', label='Mean Training Loss')\n",
    "axes[0, 0].fill_between(epochs_range,\n",
    "                       avg_train_losses - std_train_losses,\n",
    "                       avg_train_losses + std_train_losses,\n",
    "                       alpha=0.3, color='blue')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Average Training Loss Across Folds')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Average validation loss\n",
    "avg_val_losses = np.mean(fold_results['val_losses'], axis=0)\n",
    "std_val_losses = np.std(fold_results['val_losses'], axis=0)\n",
    "\n",
    "axes[0, 1].plot(epochs_range, avg_val_losses, 'r-', label='Mean Validation Loss')\n",
    "axes[0, 1].fill_between(epochs_range,\n",
    "                       avg_val_losses - std_val_losses,\n",
    "                       avg_val_losses + std_val_losses,\n",
    "                       alpha=0.3, color='red')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Average Validation Loss Across Folds')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Average training accuracy\n",
    "avg_train_acc = np.mean(fold_results['train_accuracies'], axis=0)\n",
    "std_train_acc = np.std(fold_results['train_accuracies'], axis=0)\n",
    "\n",
    "axes[1, 0].plot(epochs_range, avg_train_acc, 'g-', label='Mean Training Accuracy')\n",
    "axes[1, 0].fill_between(epochs_range,\n",
    "                       avg_train_acc - std_train_acc,\n",
    "                       avg_train_acc + std_train_acc,\n",
    "                       alpha=0.3, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "axes[1, 0].set_title('Average Training Accuracy Across Folds')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Average validation accuracy\n",
    "avg_val_acc = np.mean(fold_results['val_accuracies'], axis=0)\n",
    "std_val_acc = np.std(fold_results['val_accuracies'], axis=0)\n",
    "\n",
    "axes[1, 1].plot(epochs_range, avg_val_acc, 'm-', label='Mean Validation Accuracy')\n",
    "axes[1, 1].fill_between(epochs_range,\n",
    "                       avg_val_acc - std_val_acc,\n",
    "                       avg_val_acc + std_val_acc,\n",
    "                       alpha=0.3, color='magenta')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Average Validation Accuracy Across Folds')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for test metrics comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "metrics_names = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "metrics_means = [np.mean(test_accuracies)*100, np.mean(test_f1_scores)*100,\n",
    "                np.mean(test_precisions)*100, np.mean(test_recalls)*100]\n",
    "metrics_stds = [np.std(test_accuracies)*100, np.std(test_f1_scores)*100,\n",
    "               np.std(test_precisions)*100, np.std(test_recalls)*100]\n",
    "\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "bars = ax.bar(x_pos, metrics_means, yerr=metrics_stds, capsize=5,\n",
    "              color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score (%)')\n",
    "ax.set_title('Cross-Validation Test Metrics (Mean ± Std)')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean_val, std_val in zip(bars, metrics_means, metrics_stds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + std_val + 0.5,\n",
    "            f'{mean_val:.1f}±{std_val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ssl-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
